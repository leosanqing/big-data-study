查看JM 的日志可以看到以下的输出,这个就是 Hudi 任务运行时的全部参数配置,没有配置也会给默认配置
```properties
2023-10-09 16:14:29,908 DEBUG org.apache.hudi.sync.common.HoodieSyncConfig                 [] - Passed in properties:
archive.max_commits=50
archive.min_commits=40
cdc.enabled=false
cdc.supplemental.logging.mode=data_before_after
changelog.enabled=false
clean.async.enabled=true
clean.policy=KEEP_LATEST_COMMITS
clean.retain_commits=30
clean.retain_file_versions=5
clean.retain_hours=24
clustering.async.enabled=false
clustering.delta_commits=4
clustering.plan.partition.filter.mode=NONE
clustering.plan.strategy.class=org.apache.hudi.client.clustering.plan.strategy.FlinkSizeBasedClusteringPlanStrategy
clustering.plan.strategy.cluster.begin.partition=
clustering.plan.strategy.cluster.end.partition=
clustering.plan.strategy.daybased.lookback.partitions=2
clustering.plan.strategy.daybased.skipfromlatest.partitions=0
clustering.plan.strategy.max.num.groups=30
clustering.plan.strategy.partition.regex.pattern=
clustering.plan.strategy.partition.selected=
clustering.plan.strategy.small.file.limit=600
clustering.plan.strategy.sort.columns=
clustering.plan.strategy.target.file.max.bytes=1073741824
clustering.schedule.enabled=false
clustering.tasks=2
compaction.async.enabled=true
compaction.delta_commits=5
compaction.delta_seconds=3600
compaction.max_memory=100
compaction.schedule.enabled=true
compaction.target_io=512000
compaction.tasks=2
compaction.timeout.seconds=1200
compaction.trigger.strategy=num_commits
connector=hudi
hive_sync.assume_date_partitioning=false
hive_sync.auto_create_db=true
hive_sync.db=hudi_test
hive_sync.enable=true
hive_sync.file_format=PARQUET
hive_sync.ignore_exceptions=false
hive_sync.jdbc_url=jdbc:hive2://localhost:10000
hive_sync.metastore.uris=thrift://bigdata01:9083
hive_sync.mode=hms
hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor
hive_sync.partition_fields=
hive_sync.password=hive
hive_sync.skip_ro_suffix=false
hive_sync.support_timestamp=true
hive_sync.table=mor_1009_001_achived
hive_sync.table.strategy=ALL
hive_sync.use_jdbc=true
hive_sync.username=hive
hoodie.bucket.index.hash.field=
hoodie.bucket.index.num.buckets=4
hoodie.datasource.hive_sync.assume_date_partitioning=false
hoodie.datasource.hive_sync.auto_create_database=true
hoodie.datasource.hive_sync.base_file_format=PARQUET
hoodie.datasource.hive_sync.database=hudi_test
hoodie.datasource.hive_sync.ignore_exceptions=false
hoodie.datasource.hive_sync.jdbcurl=jdbc:hive2://localhost:10000
hoodie.datasource.hive_sync.metastore.uris=thrift://bigdata01:9083
hoodie.datasource.hive_sync.mode=hms
hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor
hoodie.datasource.hive_sync.partition_fields=partition
hoodie.datasource.hive_sync.password=hive
hoodie.datasource.hive_sync.skip_ro_suffix=false
hoodie.datasource.hive_sync.support_timestamp=true
hoodie.datasource.hive_sync.table=mor_1009_001_achived
hoodie.datasource.hive_sync.table.strategy=ALL
hoodie.datasource.hive_sync.use_jdbc=true
hoodie.datasource.hive_sync.use_pre_apache_input_format=false
hoodie.datasource.hive_sync.username=hive
hoodie.datasource.merge.type=payload_combine
hoodie.datasource.meta.sync.base.path=hdfs://bigdata01:9000/hudi_test/mor_1009_001_achived
hoodie.datasource.query.type=snapshot
hoodie.datasource.write.hive_style_partitioning=false
hoodie.datasource.write.keygenerator.type=SIMPLE
hoodie.datasource.write.partitionpath.field=partition
hoodie.datasource.write.partitionpath.urlencode=false
hoodie.datasource.write.recordkey.field=uuid
hoodie.meta.sync.decode_partition=false
hoodie.meta.sync.metadata_file_listing=false
hoodie.table.name=mor_1009_001_achived
index.bootstrap.enabled=false
index.global.enabled=true
index.partition.regex=.*
index.state.ttl=0.0
index.type=FLINK_STATE
metadata.compaction.delta_commits=10
metadata.enabled=false
partition.default_name=__HIVE_DEFAULT_PARTITION__
path=hdfs://bigdata01:9000/hudi_test/mor_1009_001_achived
payload.class=org.apache.hudi.common.model.EventTimeAvroPayload
precombine.field=ts
read.data.skipping.enabled=false
read.streaming.check-interval=60
read.streaming.enabled=false
read.streaming.skip_clustering=false
read.streaming.skip_compaction=false
read.utc-timezone=true
record.merger.impls=org.apache.hudi.common.model.HoodieAvroRecordMerger
record.merger.strategy=eeb8d96f-b1e4-49fd-bbf8-28ac514178e5
source.avro-schema={"type":"record","name":"mor_1009_001_achived_record","namespace":"hoodie.mor_1009_001_achived","fields":[{"name":"uuid","type":["null","string"],"default":null},{"name":"name","type":["null","string"],"default":null},{"name":"age","type":["null","int"],"default":null},{"name":"ts","type":["null",{"type":"long","logicalType":"timestamp-millis"}],"default":null},{"name":"partition","type":["null","string"],"default":null}]}
table.type=MERGE_ON_READ
write.batch.size=256.0
write.bucket_assign.tasks=2
write.bulk_insert.shuffle_input=true
write.bulk_insert.sort_input=true
write.commit.ack.timeout=600000
write.ignore.failed=false
write.insert.cluster=false
write.log.max.size=1024
write.log_block.size=128
write.merge.max_memory=100
write.operation=upsert
write.parquet.block.size=120
write.parquet.max.file.size=120
write.parquet.page.size=1
write.precombine=false
write.rate.limit=0
write.retry.interval.ms=2000
write.retry.times=3
write.sort.memory=128
write.task.max.size=1024.0
write.tasks=2
```